### Set current working directory.
import os
#print(os.getcwd()) # this command will show your current directory. Ensure this is consistent across all files.
path = 'C:\\Users\\Reemon Spector\\Documents\\TuringTDA'
os.chdir(path)

### Import necessary libraries, notably GUDHI and py-pde.
#import time
#t0 = time.time()
import gudhi
import pde
import numpy as np
import pandas as pd
from joblib import Parallel, delayed

### Build the grid on which the PDE (CIMA or Schnakenberg) will be solved.
def gridForPDE(Lx, Ly, stepsize=0.5):
    # Lx = width of domain.
    # Ly = height of domain.
    # stepsize = spatial discretisation stepsize, 0.5 by default.
    ### Return a 2D grid whose boundaries are not periodic.
    return pde.CartesianGrid([[0,Lx],[0,Ly]], [int(Lx/stepsize),int(Ly/stepsize)], periodic=[False, False])

### Define the class for the Schnakenberg system's PDE.
class Schnak(pde.PDEBase):
    ### Initialise the class with some parameters, the grid, the boundary conditions, and the RNG seed.
    def __init__(self, alpha, beta, delta, grid, bc="derivative", seed=1979):
        super().__init__()
        self.alpha = alpha          # production rate of u
        self.beta = beta            # production rate of v
        self.delta = delta          # diffusivity of v
        self.grid = grid            # grid generated by gridForPDE
        self.bc = bc                # boundary condition, Neumann by default
        self.seed = seed            # seed for random number generation of IC, 1979 (for Schnakenberg's paper) by default
    
    ### Define the initial conditions as a (Gaussian) perturbation of the steady state.
    def ic(self,seed=1979):
        u0 = self.alpha + self.beta + 0.1*pde.ScalarField.random_normal(self.grid, label="$u$", rng=np.random.default_rng(seed))
        v0 = self.beta / (self.alpha + self.beta)**2 + 0.1*pde.ScalarField.random_normal(self.grid, label="$v$", rng=np.random.default_rng(seed+1))
        return pde.FieldCollection([u0, v0])
    
    ### Define the RHS of the PDE
    def evolution_rate(self, state, t=0):   # optionally at time t, which by default is 0 since the PDE is autonomous.
        u,v = state                         # current values of u and v.
        du_dt = u.laplace(self.bc) + self.alpha - u + u*u*v
        dv_dt = self.delta * v.laplace(self.bc) + self.beta - u*u*v
        return pde.FieldCollection([du_dt, dv_dt])

### Define the parameters for the grid, and call the grid.
Lx = 30
Ly = 30
stepsize = 1.0
gr = gridForPDE(Lx, Ly, stepsize)

### Define some useful variables for later.
N = np.shape(gr.cell_coords)[0] * np.shape(gr.cell_coords)[1]
n = max(np.shape(gr.cell_coords)[0], np.shape(gr.cell_coords)[1])

### Construct the adjacency matrix for the gridpoints.
def nbrsForTDA(gr,stepsize):
    N = np.shape(gr.cell_coords)[0] * np.shape(gr.cell_coords)[1]   # total number of cells.
    nbrs = np.zeros([N,N])                                          # initialise empty adjacency matrix.
    cells = np.reshape(gr.cell_coords,[N,2])                        # flatten the cell coordinates into an N-by-2 matrix.
    for i in range(N):
        for j in range(N):
            if i == j:
                nbrs[i,j] = 0
            elif np.linalg.norm(cells[i]-cells[j])<1.5*stepsize:
                nbrs[i,j] = 1
    
    return nbrs

nbrs = nbrsForTDA(gr, stepsize)

### Define a 'clean-up' function to fix any buggy behaviour, and to convert infinite barcodes to ones that die at filtration value 20.
def infTo20(b,filtration_length):
    if b[1][1] > filtration_length: #i.e. if it's infinite/buggy
        return [b[1][0],float(filtration_length)]
    else:
        return [b[1][0],b[1][1]]

### Initialise a class that will compute barcodes for the Schnakenberg system.
class SchnakTDA():
    ### Initialise the class with the PDE solution, the adjacency matrix and the filtration length (which is 20 by default).
    def __init__(self, sol, nbrs, filtration_length=20):
        super().__init__()
        self.sol = sol
        self.nbrs = nbrs
        self.filtration_length = filtration_length
    
    ### Define the method that will compute the barcodes.
    def computeBarcodes(self):
        ### Start by constructing the lower-star filtration (below) and the upper-star filtration (above).
        u, v = np.reshape(self.sol.data[0],np.size(self.sol.data[0])), np.reshape(self.sol.data[1],np.size(self.sol.data[1]))   # reshape u and v appropriately.
        umin, umax, vmin, vmax = np.min(u), np.max(u), np.min(v), np.max(v)                                                     # store the min and max of u and v for later.
        ### To construct the filtration, keep track of which 0-simplices are above/below the heights dictated by the filtration value.
        #below_u, below_v = np.zeros([N,self.filtration_length]), np.zeros([N,self.filtration_length])                           # initialise the simplices below a given height.
        above_u, above_v = np.zeros([N,self.filtration_length]), np.zeros([N,self.filtration_length])                           # initialise the simplices above a given height.
        for index in range(self.filtration_length):
            h_u = umax - (index-1)*(umax-umin)/(self.filtration_length-1)                                                       # the height for u at filtration value 'index'.
            h_v = vmax - (index-1)*(vmax-vmin)/(self.filtration_length-1)                                                       # the height for v at filtration value 'index'.
            for i in range(N):
                if u[i] >= h_u:
                    above_u[i,index] = 1
                if v[i] >= h_v:
                    above_v[i,index] = 1
        
        ### Initialise empty filtrations for u and v,
        stream_u = gudhi.SimplexTree()
        stream_v = gudhi.SimplexTree()
        ### and empty vertices, edges, faces; and lists of filtration values at these v, e and f.
        V_u, fV_u, E_u, fE_u, F_u, fF_u = [], [], [], [], [], []
        V_v, fV_v, E_v, fE_v, F_v, fF_v = [], [], [], [], [], []
        for i in range(N):
            ### Add all the 0-simplices at the smallest height they occur,
            ucol_i = self.filtration_length + 1 - sum(above_u[i,:])
            V_u.append([i])
            fV_u.append(ucol_i)
            vcol_i = self.filtration_length + 1 - sum(above_v[i,:])
            V_v.append([i])
            fV_v.append(vcol_i)
            ### then check for 1-simplices,
            for j in range(max(0,i-n),min(i+n+1,N)):
                ### by first ensuring i and j are neighbouring in the grid,
                if nbrs[i,j]:
                    ### and that edges are included at the maximal filtration value of their endpoints.
                    ucol_j = self.filtration_length + 1 - sum(above_u[j,:])
                    E_u.append([i,j])
                    fE_u.append(max(ucol_i,ucol_j))
                    vcol_j = self.filtration_length + 1 - sum(above_v[j,:])
                    E_v.append([i,j])
                    fE_v.append(max(vcol_i,vcol_j))
                ### Ditto check for 2-simplices,
                for k in range(max(0,j-n,i-n),min(i+n+1,j+n+1,N)):
                    ### by ensuring all three are neighbouring,
                    if nbrs[i,j] and nbrs[j,k] and nbrs[k,i]:
                        ### and ditto including a face at the maximal filtration value of its vertices.
                        ucol_k = self.filtration_length + 1 - sum(above_u[k,:])
                        F_u.append([i,j,k])
                        fF_u.append(max(ucol_i,ucol_j,ucol_k))
                        vcol_k = self.filtration_length + 1 - sum(above_v[k,:])
                        F_v.append([i,j,k])
                        fF_v.append(max(vcol_i,vcol_j,vcol_k))
        
        ### Turn all arrays into numpy arrays for efficiency,
        V_u, fV_u, E_u, fE_u, F_u, fF_u = np.asarray(V_u), np.asarray(fV_u), np.asarray(E_u), np.asarray(fE_u), np.asarray(F_u), np.asarray(fF_u)
        V_v, fV_v, E_v, fE_v, F_v, fF_v = np.asarray(V_v), np.asarray(fV_v), np.asarray(E_v), np.asarray(fE_v), np.asarray(F_v), np.asarray(fF_v)
        ### and transpose the arrays for compatibility with the 'insert_batch' function.
        stream_u.insert_batch(np.transpose(V_u), fV_u)
        stream_u.insert_batch(np.transpose(E_u), fE_u)
        stream_u.insert_batch(np.transpose(F_u), fF_u)
        stream_v.insert_batch(np.transpose(V_v), fV_v)
        stream_v.insert_batch(np.transpose(E_v), fE_v)
        stream_v.insert_batch(np.transpose(F_v), fF_v)
        ### Compute the barcodes, and return as lists.
        barcodes_u = stream_u.persistence()
        barcodes_v = stream_v.persistence()
        u0 = [infTo20(barcodes_u[i],self.filtration_length) for i in range(len(barcodes_u)) if barcodes_u[i][0]==0]
        u1 = [infTo20(barcodes_u[i],self.filtration_length) for i in range(len(barcodes_u)) if barcodes_u[i][0]==1]
        v0 = [infTo20(barcodes_v[i],self.filtration_length) for i in range(len(barcodes_v)) if barcodes_v[i][0]==0]
        v1 = [infTo20(barcodes_v[i],self.filtration_length) for i in range(len(barcodes_v)) if barcodes_v[i][0]==1]
        return np.asarray([u0,u1,v0,v1],list)

### Initialise the nodes of the discretised Turing space.
meshNodes = []
fineness = 16
D = np.linspace(25,45,1+fineness//2)
A = np.linspace(0.001,0.5,fineness)
B = np.linspace(0.001,3,fineness)
for d in D:
    for a in A:
        for b in B:
            u,v = a+b, b/(a+b)**2                           # set u and v as the steady state.
            fu = (b-a)/(b+a)
            fv = (a+b)**2
            gu = -2*b/(a+b)
            gv =  -(a+b)**2
            C1 = (fu + gv < 0)
            C2 = (fu*gv - fv*gu > 0)
            C3 = (d*fu + gv > 0)
            C4 = ((d*fu + gv)**2 - 4*d*(fu*gv - fv*gu) > 0)
            if C1 and C2 and C3 and C4:
                meshNodes.append([a,b,d])                   # if a node satisfies all four conditions, add it to the list of nodes.

meshNodes = np.asarray(meshNodes)
#print(np.shape(meshNodes))

### Define a function that will solve the PDEs in parallel.
def parallelBarcodes(node,gr,index):
    a,b,d = node
    eq = Schnak(a,b,d,gr)
    trackers = [
        "progress",         # show progress bar during simulation.
        "steady_state",     # abort when steady state is reached.
    ]
    ### Solve the equation with the random initial conditions, until time 500s or a steady state is reached, with initial time step 0.001 and adaptive timestepping.
    sol = eq.solve(state=eq.ic(), t_range=500, dt=0.001, adaptive=True, tracker=trackers)
    ### Plot and save the final value of u.
    sol[0].plot(title='Final $u$ value at [$\\alpha$,$\\beta$,$\\delta$] = '+str([round(a,2),round(b,2),round(d,2)]),
                 cmap='turbo', filename=path+'\\Schnakpythonfigs\\'+str(index+1)+'.png', action='close')
    ### Return the barcodes of u and v at the end.
    return SchnakTDA(sol,nbrs).computeBarcodes()

#t1 = time.time()
### Compute the barcodes with 6 concurrent jobs (this was slightly faster than 8 -- the total number of cores -- concurrent job when tested on a subset of the nodes)
barcodes = Parallel(n_jobs=6)(delayed(parallelBarcodes)(meshNodes[index],gr,index) for index in range(len(meshNodes)))
#print(str(int((time.time()-t1)))+"s total")
#print(str(int((time.time()-t1)/len(meshNodes)))+"s per node")

### Write the nodes, and barcodes to .CSVs for later analysis.
dfNodes = pd.DataFrame(meshNodes)
dfNodes.to_csv("Schnaknodes.csv",index=False,header=["a","b","d"])

dfBarcodes = pd.DataFrame(barcodes)
dfBarcodes.to_csv("Schnakbarcodes.csv",index=False,header=["u0","v0","u1","v1"],sep=',')


### Code to decode the barcodes from strings into lists.
# import json
# csvBarcodes = pd.read_csv("CIMAbarcodes2.csv")
# csvnpBarcodes = csvBarcodes.to_numpy()
# barcodes2 = np.array([json.loads(csvnpBarcodes[i][j]) for i in range(len(csvnpBarcodes)) for j in range(4)],list)